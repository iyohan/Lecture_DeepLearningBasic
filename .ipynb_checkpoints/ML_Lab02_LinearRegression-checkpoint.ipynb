{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis and cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H(x) = Wx + b  \n",
    "cost(W, b) = 1/m * sig(i=1, m)(H(x_i) - y_i^2)  \n",
    "  \n",
    "학습이란? cost function을 minimize하는 것!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build graph using TF operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H(x) = Wx + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# X and Y data\n",
    "x_train = [1, 2, 3]\n",
    "y_train = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Our hypothesis Wx + b\n",
    "hypothesis = x_train * W + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 Variable은 일반적인 변수와는 다르다.  \n",
    "텐서플로가 학습하면서 바꾸는 변수로, **trainable variable**이라고도 불린다.  \n",
    "  \n",
    "값을 모르기 때문에, 먼저 랜덤으로 생성하고, shape을 [1]로 해줌."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cost(W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduce_mean()은 어떤 텐서가 있을 때, 그것의 평균을 구해준다.  \n",
    "  \n",
    "t = [1., 2., 3., 4.]  \n",
    "tf.reduce_mean(t) ==> 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5\n"
     ]
    }
   ],
   "source": [
    "# reduce_mean의 예.\n",
    "t = [1., 2., 3., 4.]\n",
    "test = tf.Session()\n",
    "print(test.run(tf.reduce_mean(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)  # cost minimize 하는 node(이름: train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run/update graph and get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.0343137 [-0.2310593] [0.45613366]\n",
      "20 0.13577645 [0.56735367] [0.76046973]\n",
      "40 0.08309016 [0.6582018] [0.7557501]\n",
      "60 0.07509915 [0.6809795] [0.7231864]\n",
      "80 0.06820291 [0.6966116] [0.6894805]\n",
      "100 0.061942935 [0.71093065] [0.65710425]\n",
      "120 0.056257594 [0.7245216] [0.6262254]\n",
      "140 0.051094055 [0.7374686] [0.59679544]\n",
      "160 0.04640447 [0.7498066] [0.5687483]\n",
      "180 0.042145286 [0.7615648] [0.54201925]\n",
      "200 0.03827702 [0.7727704] [0.51654637]\n",
      "220 0.0347638 [0.78344935] [0.49227053]\n",
      "240 0.031573024 [0.7936264] [0.4691356]\n",
      "260 0.028675111 [0.80332524] [0.44708797]\n",
      "280 0.026043212 [0.81256825] [0.42607647]\n",
      "300 0.023652857 [0.8213768] [0.40605247]\n",
      "320 0.02148191 [0.8297715] [0.3869695]\n",
      "340 0.0195102 [0.83777153] [0.36878338]\n",
      "360 0.017719494 [0.8453957] [0.35145196]\n",
      "380 0.01609315 [0.8526615] [0.334935]\n",
      "400 0.014616046 [0.8595858] [0.31919435]\n",
      "420 0.01327452 [0.8661848] [0.30419344]\n",
      "440 0.01205615 [0.87247366] [0.2898975]\n",
      "460 0.010949586 [0.87846684] [0.27627334]\n",
      "480 0.009944584 [0.8841785] [0.26328945]\n",
      "500 0.009031833 [0.8896217] [0.25091583]\n",
      "520 0.008202844 [0.89480907] [0.23912373]\n",
      "540 0.0074499473 [0.89975274] [0.22788577]\n",
      "560 0.006766165 [0.9044639] [0.21717596]\n",
      "580 0.0061451388 [0.9089538] [0.20696948]\n",
      "600 0.005581111 [0.9132326] [0.19724266]\n",
      "620 0.0050688544 [0.9173103] [0.18797302]\n",
      "640 0.0046036267 [0.9211964] [0.17913902]\n",
      "660 0.004181094 [0.92489994] [0.17072016]\n",
      "680 0.0037973325 [0.9284293] [0.16269696]\n",
      "700 0.0034488002 [0.9317929] [0.15505077]\n",
      "720 0.0031322467 [0.93499845] [0.1477639]\n",
      "740 0.002844752 [0.9380532] [0.14081953]\n",
      "760 0.0025836478 [0.9409645] [0.13420153]\n",
      "780 0.0023465126 [0.943739] [0.12789457]\n",
      "800 0.002131142 [0.94638306] [0.12188397]\n",
      "820 0.0019355394 [0.9489029] [0.11615588]\n",
      "840 0.0017578829 [0.95130426] [0.11069695]\n",
      "860 0.0015965427 [0.9535927] [0.10549458]\n",
      "880 0.0014500035 [0.9557737] [0.10053673]\n",
      "900 0.0013169158 [0.9578521] [0.0958119]\n",
      "920 0.0011960474 [0.9598331] [0.0913091]\n",
      "940 0.001086268 [0.9617207] [0.08701786]\n",
      "960 0.0009865599 [0.9635197] [0.08292834]\n",
      "980 0.0008960147 [0.9652341] [0.079031]\n",
      "1000 0.0008137715 [0.966868] [0.07531683]\n",
      "1020 0.00073908083 [0.9684251] [0.07177722]\n",
      "1040 0.0006712495 [0.96990895] [0.06840403]\n",
      "1060 0.0006096343 [0.97132313] [0.06518927]\n",
      "1080 0.00055368064 [0.97267085] [0.06212561]\n",
      "1100 0.0005028616 [0.9739552] [0.05920593]\n",
      "1120 0.00045671043 [0.9751792] [0.05642349]\n",
      "1140 0.00041479166 [0.9763457] [0.05377179]\n",
      "1160 0.00037672036 [0.9774574] [0.0512447]\n",
      "1180 0.0003421407 [0.9785168] [0.04883637]\n",
      "1200 0.00031073796 [0.9795265] [0.04654125]\n",
      "1220 0.00028221638 [0.9804887] [0.04435392]\n",
      "1240 0.00025631534 [0.9814056] [0.04226943]\n",
      "1260 0.00023278804 [0.9822795] [0.04028291]\n",
      "1280 0.00021142303 [0.9831123] [0.03838976]\n",
      "1300 0.00019201824 [0.983906] [0.03658558]\n",
      "1320 0.00017439241 [0.98466235] [0.03486615]\n",
      "1340 0.00015838548 [0.98538315] [0.03322754]\n",
      "1360 0.00014384829 [0.9860701] [0.03166596]\n",
      "1380 0.000130645 [0.9867248] [0.03017778]\n",
      "1400 0.00011865442 [0.9873486] [0.02875955]\n",
      "1420 0.000107764565 [0.9879432] [0.02740797]\n",
      "1440 9.787263e-05 [0.9885098] [0.02611992]\n",
      "1460 8.889133e-05 [0.9890498] [0.0248924]\n",
      "1480 8.073278e-05 [0.9895643] [0.02372261]\n",
      "1500 7.332117e-05 [0.99005485] [0.02260774]\n",
      "1520 6.6593035e-05 [0.99052227] [0.02154525]\n",
      "1540 6.0480947e-05 [0.99096763] [0.0205327]\n",
      "1560 5.492779e-05 [0.9913922] [0.01956772]\n",
      "1580 4.988699e-05 [0.9917967] [0.01864811]\n",
      "1600 4.5309145e-05 [0.9921822] [0.01777172]\n",
      "1620 4.1149557e-05 [0.9925496] [0.01693652]\n",
      "1640 3.737285e-05 [0.9928997] [0.01614055]\n",
      "1660 3.3942764e-05 [0.99323344] [0.01538201]\n",
      "1680 3.0827156e-05 [0.99355143] [0.0146591]\n",
      "1700 2.799758e-05 [0.9938545] [0.01397016]\n",
      "1720 2.5427957e-05 [0.9941433] [0.01331362]\n",
      "1740 2.3094466e-05 [0.9944185] [0.01268796]\n",
      "1760 2.0974387e-05 [0.9946809] [0.01209168]\n",
      "1780 1.9049136e-05 [0.9949308] [0.01152343]\n",
      "1800 1.7301276e-05 [0.9951691] [0.01098187]\n",
      "1820 1.5712807e-05 [0.99539614] [0.01046575]\n",
      "1840 1.4271169e-05 [0.9956125] [0.00997387]\n",
      "1860 1.2960874e-05 [0.9958187] [0.00950513]\n",
      "1880 1.17711825e-05 [0.9960152] [0.00905842]\n",
      "1900 1.0690771e-05 [0.99620247] [0.00863271]\n",
      "1920 9.709268e-06 [0.9963809] [0.008227]\n",
      "1940 8.818378e-06 [0.996551] [0.00784036]\n",
      "1960 8.0092095e-06 [0.9967131] [0.00747191]\n",
      "1980 7.27397e-06 [0.9968676] [0.00712074]\n",
      "2000 6.606009e-06 [0.9970148] [0.00678609]\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph 필수!!\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Fit the line\n",
    "for step in range(2001):\n",
    "    sess.run(train)  # train이라는 node를 실행.\n",
    "    if step % 20 == 0:\n",
    "        print(step, sess.run(cost), sess.run(W), sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholder를 이용해서 train을 할 때 그때 넘겨줄 수 있다.  \n",
    "먼저 학습 모델을 만들고, 필요할때 feed할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 77.18251 [-0.7751612]\n",
      "20 0.22838017 [-0.01936033]\n",
      "40 0.1980757 [0.05680654]\n",
      "60 0.17298093 [0.12513897]\n",
      "80 0.15106562 [0.18898343]\n",
      "100 0.13192673 [0.24864662]\n",
      "120 0.115212634 [0.30440238]\n",
      "140 0.100616 [0.3565067]\n",
      "160 0.08786872 [0.4051986]\n",
      "180 0.076736435 [0.45070174]\n",
      "200 0.06701444 [0.4932248]\n",
      "220 0.058524262 [0.5329629]\n",
      "240 0.05110968 [0.5700987]\n",
      "260 0.044634435 [0.6048023]\n",
      "280 0.038979597 [0.63723326]\n",
      "300 0.034041174 [0.6675401]\n",
      "320 0.029728403 [0.6958623]\n",
      "340 0.025962058 [0.7223295]\n",
      "360 0.022672873 [0.74706346]\n",
      "380 0.019800344 [0.7701776]\n",
      "400 0.017291794 [0.791778]\n",
      "420 0.015101066 [0.8119636]\n",
      "440 0.013187843 [0.8308275]\n",
      "460 0.011517057 [0.8484558]\n",
      "480 0.010057925 [0.8649296]\n",
      "500 0.008783644 [0.88032454]\n",
      "520 0.007670842 [0.8947114]\n",
      "540 0.0066989944 [0.90815586]\n",
      "560 0.005850298 [0.92071986]\n",
      "580 0.005109092 [0.9324611]\n",
      "600 0.0044618137 [0.94343334]\n",
      "620 0.0038965344 [0.95368683]\n",
      "640 0.0034028825 [0.9632689]\n",
      "660 0.002971754 [0.9722235]\n",
      "680 0.002595275 [0.98059165]\n",
      "700 0.002266473 [0.9884117]\n",
      "720 0.0019793217 [0.99571985]\n",
      "740 0.00172855 [1.0025493]\n",
      "760 0.0015095556 [1.0089314]\n",
      "780 0.001318302 [1.0148956]\n",
      "800 0.0011512993 [1.020469]\n",
      "820 0.0010054316 [1.0256773]\n",
      "840 0.0008780432 [1.0305452]\n",
      "860 0.0007667962 [1.0350939]\n",
      "880 0.0006696441 [1.0393449]\n",
      "900 0.0005848067 [1.0433173]\n",
      "920 0.00051071076 [1.0470296]\n",
      "940 0.00044601347 [1.0504985]\n",
      "960 0.00038950823 [1.0537401]\n",
      "980 0.00034016668 [1.0567697]\n",
      "1000 0.0002970682 [1.0596007]\n",
      "1020 0.00025943518 [1.0622463]\n",
      "1040 0.00022656618 [1.0647191]\n",
      "1060 0.00019786165 [1.0670297]\n",
      "1080 0.00017279071 [1.0691891]\n",
      "1100 0.00015089876 [1.0712068]\n",
      "1120 0.00013178101 [1.0730926]\n",
      "1140 0.00011508622 [1.0748547]\n",
      "1160 0.00010050542 [1.0765015]\n",
      "1180 8.777472e-05 [1.0780404]\n",
      "1200 7.6651006e-05 [1.0794785]\n",
      "1220 6.694153e-05 [1.0808223]\n",
      "1240 5.8461046e-05 [1.0820783]\n",
      "1260 5.1052746e-05 [1.0832521]\n",
      "1280 4.458541e-05 [1.0843488]\n",
      "1300 3.893734e-05 [1.0853736]\n",
      "1320 3.4005447e-05 [1.0863315]\n",
      "1340 2.9697756e-05 [1.0872266]\n",
      "1360 2.593471e-05 [1.0880631]\n",
      "1380 2.2649121e-05 [1.0888449]\n",
      "1400 1.9779935e-05 [1.0895756]\n",
      "1420 1.7271008e-05 [1.0902587]\n",
      "1440 1.5083276e-05 [1.0908967]\n",
      "1460 1.3173338e-05 [1.0914929]\n",
      "1480 1.1503509e-05 [1.09205]\n",
      "1500 1.004682e-05 [1.0925704]\n",
      "1520 8.7736e-06 [1.093057]\n",
      "1540 7.662201e-06 [1.0935118]\n",
      "1560 6.6909633e-06 [1.0939368]\n",
      "1580 5.8430387e-06 [1.0943339]\n",
      "1600 5.1034194e-06 [1.094705]\n",
      "1620 4.456016e-06 [1.0950519]\n",
      "1640 3.891617e-06 [1.0953759]\n",
      "1660 3.3988722e-06 [1.0956786]\n",
      "1680 2.968407e-06 [1.0959615]\n",
      "1700 2.5927345e-06 [1.0962257]\n",
      "1720 2.264265e-06 [1.0964729]\n",
      "1740 1.9777137e-06 [1.0967038]\n",
      "1760 1.7269798e-06 [1.0969197]\n",
      "1780 1.5083633e-06 [1.0971212]\n",
      "1800 1.317381e-06 [1.0973098]\n",
      "1820 1.1503007e-06 [1.0974858]\n",
      "1840 1.0049067e-06 [1.0976504]\n",
      "1860 8.7743155e-07 [1.0978043]\n",
      "1880 7.6630766e-07 [1.0979481]\n",
      "1900 6.6916823e-07 [1.0980823]\n",
      "1920 5.8452076e-07 [1.0982077]\n",
      "1940 5.106304e-07 [1.098325]\n",
      "1960 4.4593102e-07 [1.0984347]\n",
      "1980 3.8947434e-07 [1.0985371]\n",
      "2000 3.4017307e-07 [1.0986326]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# X and Y data\n",
    "#x_train = [1, 2, 3, 4, 5]\n",
    "#y_train = [2.1, 3.1, 4.1, 5.1, 6.1]\n",
    "\n",
    "# Now we can use X and Y i place of x_data and y_data\n",
    "# # placegolders for a tensor that will be always fed using feed_dict\n",
    "# See http://stackoverflow.com/questions/36693740/\n",
    "X = tf.placeholder(tf.float32, shape=[None])\n",
    "Y = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Our hypothesis Wx + b\n",
    "hypothesis = X * W + b\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)  # cost minimize 하는 node(이름: train) \n",
    "\n",
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph 필수!!\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Fit the line\n",
    "for step in range(2001):\n",
    "    cost_val, W_val, b_val, _ = \\\n",
    "        sess.run([cost, W, b, train],\n",
    "                feed_dict={X: [1, 2, 3, 4, 5], \n",
    "                           Y: [2.1, 3.1, 4.1, 5.1, 6.1]})\n",
    "    if step % 20 == 0:\n",
    "        print(step, cost_val, b_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
